{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1979fb",
   "metadata": {},
   "source": [
    "# Problem 1 (60 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084ba03",
   "metadata": {},
   "source": [
    "In this assignment, you will use a transformer encoder to implement static embeddings (like\n",
    "word2vec). The method we will use comes from this paper: https://aclanthology.org/2020.acl-\n",
    "main.431.pdf\n",
    "Please implement the algorithm below:\n",
    "1. Choose the transformer encoder you will work with. For example, these are good\n",
    "encoder models:  \n",
    " a. https://huggingface.co/FacebookAI/roberta-base  \n",
    " b. https://huggingface.co/microsoft/deberta-v3-base  \n",
    " c. https://huggingface.co/Tejas3/distillbert_base_uncased_80_equal  \n",
    "2. Read the texts provided with this assignment in the dataset uploaded in D2L under\n",
    "Content / Assignments / assignment4-dataset.txt.gz. This dataset contains one sentence\n",
    "per line. Tokenize each individual sentence using the tokenizer corresponding to the\n",
    "transformer chosen in the previous step. Note that the resulting tokens are sub-word\n",
    "tokens that may not correspond to a full word.\n",
    "3. Generate the contextualized embeddings for all the tokens in the dataset and compute\n",
    "the average embedding for each token in the vocabulary by averaging all its\n",
    "contextualized embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc7c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9eedd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\26653\\.conda\\envs\\torch-nightly\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gzip\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e1807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Hidden dimension: 768, Vocab size: 50265\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Choose the transformer encoder\n",
    "MODEL_NAME = \"roberta-base\" \n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval() # Set model to evaluation mode (disables dropout)\n",
    "\n",
    "# Get hidden size \n",
    "hidden_dim = model.config.hidden_size\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"Model loaded. Hidden dimension: {hidden_dim}, Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153104ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found saved embeddings file: 'roberta_static_embeddings.pt'\n",
      "Loading directly from disk (Skipping computation)...\n",
      "------------------------------\n",
      "Final Embeddings Shape: torch.Size([50265, 768])\n",
      "Ready for next steps.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gzip\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Define a Dataset Class for Fast Loading ---\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.lines = []\n",
    "        open_func = gzip.open if file_path.endswith(\".gz\") else open\n",
    "        print(f\"Loading {file_path} into memory...\")\n",
    "        \n",
    "        # Read all lines into memory\n",
    "        with open_func(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "            self.lines = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        # OPTIMIZATION: Sort by length\n",
    "        # This groups sentences of similar length to minimize padding overhead\n",
    "        print(\"Sorting dataset by length for efficiency...\")\n",
    "        self.lines.sort(key=len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lines[idx]\n",
    "\n",
    "# --- 2. The Optimized Function ---\n",
    "def compute_static_embeddings_fast(file_path, model, tokenizer, device, batch_size=256):\n",
    "    dataset = TextDataset(file_path)\n",
    "    \n",
    "    # num_workers=0 to avoid multiprocessing overhead\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    token_embedding_sums = torch.zeros((tokenizer.vocab_size, model.config.hidden_size), device=device)\n",
    "    token_counts = torch.zeros(tokenizer.vocab_size, device=device)\n",
    "\n",
    "    print(f\"Starting High-Performance Inference on {device}...\")\n",
    "    print(f\"Batch Size: {batch_size} | Mixed Precision: ON\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for batch_lines in tqdm(loader, unit=\"batch\"):\n",
    "        inputs = tokenizer(\n",
    "            batch_lines, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(**inputs)\n",
    "        \n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        input_ids = inputs.input_ids\n",
    "        \n",
    "        # Vectorized Accumulation\n",
    "        flat_ids = input_ids.view(-1)\n",
    "        flat_vectors = last_hidden_state.view(-1, model.config.hidden_size)\n",
    "        \n",
    "        mask = flat_ids != tokenizer.pad_token_id\n",
    "        clean_ids = flat_ids[mask]\n",
    "        clean_vectors = flat_vectors[mask]\n",
    "        \n",
    "        token_embedding_sums.index_add_(0, clean_ids, clean_vectors.to(torch.float32))\n",
    "        \n",
    "        ones = torch.ones_like(clean_ids, dtype=torch.float)\n",
    "        token_counts.index_add_(0, clean_ids, ones)\n",
    "\n",
    "    return token_embedding_sums, token_counts\n",
    "\n",
    "# --- 3. EXECUTION LOGIC (Smart Caching) ---\n",
    "DATASET_PATH = \"assignment4-dataset.txt\" \n",
    "OUTPUT_FILE = \"roberta_static_embeddings.pt\"\n",
    "BATCH_SIZE = 256 \n",
    "\n",
    "# Check if file exists first\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    print(f\"✅ Found saved embeddings file: '{OUTPUT_FILE}'\")\n",
    "    print(\"Loading directly from disk (Skipping computation)...\")\n",
    "    static_embeddings = torch.load(OUTPUT_FILE, map_location=device)\n",
    "\n",
    "else:\n",
    "    print(f\"❌ File '{OUTPUT_FILE}' not found. Starting computation...\")\n",
    "    \n",
    "    # Run the fast function\n",
    "    sums, counts = compute_static_embeddings_fast(DATASET_PATH, model, tokenizer, device, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Compute Average\n",
    "    safe_counts = counts.clamp(min=1).unsqueeze(1)\n",
    "    static_embeddings = sums / safe_counts\n",
    "    \n",
    "    # Zero out unseen tokens\n",
    "    mask = (counts > 0).unsqueeze(1)\n",
    "    static_embeddings = static_embeddings * mask\n",
    "    \n",
    "    # Save to file\n",
    "    print(f\"Saving computed embeddings to '{OUTPUT_FILE}'...\")\n",
    "    torch.save(static_embeddings, OUTPUT_FILE)\n",
    "\n",
    "# Final Sanity Check\n",
    "print(\"-\" * 30)\n",
    "print(f\"Final Embeddings Shape: {static_embeddings.shape}\")\n",
    "print(\"Ready for next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ef6a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed static embeddings matrix shape: torch.Size([50265, 768])\n"
     ]
    }
   ],
   "source": [
    "# Avoid division by zero for tokens that never appeared in the dataset\n",
    "# We create a clamp to ensure we don't divide by 0 (replace 0 with 1 temporarily)\n",
    "safe_counts = counts.clamp(min=1).unsqueeze(1)\n",
    "\n",
    "# Compute averages\n",
    "static_embeddings = sums / safe_counts\n",
    "\n",
    "# Zero out embeddings for tokens that were never seen (optional cleanup)\n",
    "# This ensures unseen tokens are exactly 0 vector rather than a calculation artifact\n",
    "mask = (counts > 0).unsqueeze(1)\n",
    "static_embeddings = static_embeddings * mask\n",
    "\n",
    "print(f\"Computed static embeddings matrix shape: {static_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f346b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for 'film': 21928\n",
      "Times seen in dataset: 788\n",
      "Embedding vector (first 10 dims): [ 0.05221577  0.03705933 -0.01913649 -0.15739763 -0.14939709 -0.0253762\n",
      "  0.04949529  0.12681545  0.035868    0.05667021]\n",
      "Success! We have a non-zero static embedding.\n"
     ]
    }
   ],
   "source": [
    "# Helper function to get embedding for a word\n",
    "def get_static_embedding(word):\n",
    "    # Note: Tokenizers might split words or add prefixes (like Ġ in RoBERTa)\n",
    "    # This is a simplified check for exact token matches\n",
    "    ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "    if len(ids) == 1:\n",
    "        token_id = ids[0]\n",
    "        count = counts[token_id].item()\n",
    "        vector = static_embeddings[token_id]\n",
    "        return token_id, count, vector\n",
    "    else:\n",
    "        print(f\"Word '{word}' splits into multiple tokens: {ids}. Checking first token.\")\n",
    "        token_id = ids[0]\n",
    "        count = counts[token_id].item()\n",
    "        vector = static_embeddings[token_id]\n",
    "        return token_id, count, vector\n",
    "\n",
    "t_id, t_count, t_vec = get_static_embedding(\"film\")\n",
    "\n",
    "print(f\"Token ID for 'film': {t_id}\")\n",
    "print(f\"Times seen in dataset: {int(t_count)}\")\n",
    "print(f\"Embedding vector (first 10 dims): {t_vec[:10].cpu().numpy()}\")\n",
    "\n",
    "# Check if the vector is not all zeros\n",
    "if torch.count_nonzero(t_vec) > 0:\n",
    "    print(\"Success! We have a non-zero static embedding.\")\n",
    "else:\n",
    "    print(\"Warning: This token was not found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff7f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for 'film':\n",
      " - film (Similarity: 1.0000)\n",
      " - film (Similarity: 0.9594)\n",
      " - movie (Similarity: 0.9590)\n",
      " - Film (Similarity: 0.9582)\n",
      " - artist (Similarity: 0.9509)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def find_nearest_neighbors(query_word, static_embeddings, tokenizer, top_k=5):\n",
    "    # 1. Get the ID for the query word\n",
    "    ids = tokenizer.encode(query_word, add_special_tokens=False)\n",
    "    if not ids: return\n",
    "    query_id = ids[0]\n",
    "    \n",
    "    # 2. Get the vector for the query word\n",
    "    query_vec = static_embeddings[query_id].unsqueeze(0) # Shape: [1, 768]\n",
    "    \n",
    "    # 3. Calculate Cosine Similarity against ALL other words in vocab \n",
    "    sims = F.cosine_similarity(query_vec, static_embeddings)\n",
    "    \n",
    "    # 4. Get the top K matches\n",
    "    top_k_values, top_k_indices = torch.topk(sims, top_k)\n",
    "    \n",
    "    print(f\"Nearest neighbors for '{query_word}':\")\n",
    "    for value, idx in zip(top_k_values, top_k_indices):\n",
    "        word = tokenizer.decode([idx]).strip()\n",
    "        print(f\" - {word} (Similarity: {value.item():.4f})\")\n",
    "\n",
    "# Run the test\n",
    "find_nearest_neighbors(\"film\", static_embeddings, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f39123b",
   "metadata": {},
   "source": [
    "# Problem 2 (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b5b85",
   "metadata": {},
   "source": [
    "Implement the most_similar() function from the chapter 9 code, and use it to run the six\n",
    "examples in the notebook. Include the output of these calls in your notebook.\n",
    "IMPORTANT NOTE: the most_similar() function operates over actual words, whereas the\n",
    "embeddings you computed in problem 1 operate over transformer tokens. That is, each English\n",
    "word may consist of one or more tokens. To aggregate token embeddings into word\n",
    "embeddings, implement the following algorithm:  \n",
    "  1. Take the glove_vocabulary.txt (available in D2L under Content / Assignments) file and\n",
    "tokenize all the words in this file using the same tokenizer you used in the previous\n",
    "problem.  \n",
    "  2. Compute a word embedding for all words in this file by averaging the corresponding\n",
    "token embeddings.\n",
    "Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fffbd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\26653\\.conda\\envs\\torch-nightly\\lib\\site-packages (from gensim) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\26653\\.conda\\envs\\torch-nightly\\lib\\site-packages (from gensim) (1.16.2)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Downloading wrapt-2.0.1-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Downloading gensim-4.4.0-cp312-cp312-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 3.9/24.4 MB 18.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 7.3/24.4 MB 17.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 17.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.8/24.4 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.9/24.4 MB 16.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 16.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.4 MB 14.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 14.3 MB/s  0:00:01\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading wrapt-2.0.1-cp312-cp312-win_amd64.whl (60 kB)\n",
      "Installing collected packages: wrapt, smart_open, gensim\n",
      "\n",
      "   ------------- -------------------------- 1/3 [smart_open]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   ---------------------------------------- 3/3 [gensim]\n",
      "\n",
      "Successfully installed gensim-4.4.0 smart_open-7.5.0 wrapt-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70ddd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe model... this may take a minute.\n",
      "Model loaded successfully!\n",
      "----------------------------------------\n",
      "Top 10 words most similar to 'cactus':\n",
      "  cacti: 0.6635\n",
      "  saguaro: 0.6196\n",
      "  pear: 0.5233\n",
      "  cactuses: 0.5178\n",
      "  prickly: 0.5156\n",
      "  mesquite: 0.4845\n",
      "  opuntia: 0.4540\n",
      "  shrubs: 0.4536\n",
      "  peyote: 0.4534\n",
      "  succulents: 0.4513\n",
      "----------------------------------------\n",
      "Top 10 words most similar to 'cake':\n",
      "  cakes: 0.7506\n",
      "  chocolate: 0.6966\n",
      "  dessert: 0.6440\n",
      "  pie: 0.6087\n",
      "  cookies: 0.6082\n",
      "  frosting: 0.6017\n",
      "  bread: 0.5955\n",
      "  cookie: 0.5934\n",
      "  recipe: 0.5827\n",
      "  baked: 0.5820\n",
      "----------------------------------------\n",
      "Top 10 words most similar to 'angry':\n",
      "  enraged: 0.7088\n",
      "  furious: 0.7078\n",
      "  irate: 0.6939\n",
      "  outraged: 0.6705\n",
      "  frustrated: 0.6516\n",
      "  angered: 0.6353\n",
      "  provoked: 0.5827\n",
      "  annoyed: 0.5819\n",
      "  incensed: 0.5752\n",
      "  indignant: 0.5704\n",
      "----------------------------------------\n",
      "Top 10 words most similar to 'quickly':\n",
      "  soon: 0.7662\n",
      "  rapidly: 0.7217\n",
      "  swiftly: 0.7197\n",
      "  eventually: 0.7043\n",
      "  finally: 0.6901\n",
      "  immediately: 0.6843\n",
      "  then: 0.6697\n",
      "  slowly: 0.6646\n",
      "  gradually: 0.6402\n",
      "  when: 0.6348\n",
      "----------------------------------------\n",
      "Top 10 words most similar to 'between':\n",
      "  sides: 0.5868\n",
      "  both: 0.5843\n",
      "  two: 0.5652\n",
      "  differences: 0.5141\n",
      "  which: 0.5120\n",
      "  conflict: 0.5115\n",
      "  relationship: 0.5023\n",
      "  and: 0.4984\n",
      "  in: 0.4971\n",
      "  relations: 0.4970\n",
      "----------------------------------------\n",
      "Top 10 words most similar to 'the':\n",
      "  of: 0.7058\n",
      "  which: 0.6992\n",
      "  this: 0.6747\n",
      "  part: 0.6727\n",
      "  same: 0.6592\n",
      "  its: 0.6447\n",
      "  first: 0.6399\n",
      "  in: 0.6361\n",
      "  one: 0.6245\n",
      "  that: 0.6176\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Load the GloVe Model\n",
    "# ---------------------------------------------------------\n",
    "# We need to load the model again because variables don't transfer between notebooks.\n",
    "print(\"Loading GloVe model... this may take a minute.\")\n",
    "fname = \"glove.6B.300d.txt\"\n",
    "\n",
    "# Load the vectors (no header for GloVe files)\n",
    "glove = KeyedVectors.load_word2vec_format(fname, binary=False, no_header=True)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Setup Variables\n",
    "# ---------------------------------------------------------\n",
    "# Get the normalized vectors for cosine similarity\n",
    "vectors = glove.get_normed_vectors()\n",
    "index_to_key = glove.index_to_key\n",
    "key_to_index = glove.key_to_index\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Define the most_similar_words function (from Chapter 9)\n",
    "# ---------------------------------------------------------\n",
    "def most_similar_words(word, vectors, index_to_key, key_to_index, topn=10):\n",
    "    # retrieve word_id corresponding to given word\n",
    "    if word not in key_to_index:\n",
    "        return [] # Return empty if word not found\n",
    "        \n",
    "    word_id = key_to_index[word]\n",
    "    \n",
    "    # retrieve embedding for given word\n",
    "    emb = vectors[word_id]\n",
    "    \n",
    "    # calculate similarities to all words in our vocabulary\n",
    "    similarities = vectors @ emb\n",
    "    \n",
    "    # get word_ids in ascending order with respect to similarity score\n",
    "    ids_ascending = similarities.argsort()\n",
    "    \n",
    "    # reverse word_ids to get descending order\n",
    "    ids_descending = ids_ascending[::-1]\n",
    "    \n",
    "    # remove the word itself from the results\n",
    "    mask = ids_descending != word_id\n",
    "    ids_descending = ids_descending[mask]\n",
    "    \n",
    "    # get topn word_ids\n",
    "    top_ids = ids_descending[:topn]\n",
    "    \n",
    "    # retrieve topn words with their corresponding similarity score\n",
    "    top_words = [(index_to_key[i], similarities[i]) for i in top_ids]\n",
    "    \n",
    "    return top_words\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Execute the \"Six Examples\" from the Notebook\n",
    "# ---------------------------------------------------------\n",
    "# These are the 6 examples found in the original Chapter 9 notebook\n",
    "examples = [\"cactus\", \"cake\", \"angry\", \"quickly\", \"between\", \"the\"]\n",
    "\n",
    "print(\"-\" * 40)\n",
    "for example in examples:\n",
    "    print(f\"Top 10 words most similar to '{example}':\")\n",
    "    results = most_similar_words(example, vectors, index_to_key, key_to_index)\n",
    "    \n",
    "    if not results:\n",
    "        print(f\"  Word '{example}' not found in vocabulary.\")\n",
    "    else:\n",
    "        for word, score in results:\n",
    "            print(f\"  {word}: {score:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b17a75",
   "metadata": {},
   "source": [
    "# Problem 3 (30 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e8cf9",
   "metadata": {},
   "source": [
    "We have three tokens in the sentence **“bagel with cheese”**:\n",
    "\n",
    "- $w_1 =$ *bagel*\n",
    "- $w_2 =$ *with*\n",
    "- $w_3 =$ *cheese*\n",
    "\n",
    "with query, key, value vectors\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_1 &= [1,2,3], & k_1 &= [1,1,1], & v_1 &= [2,0,1],\\\\\n",
    "q_2 &= [2,3,2], & k_2 &= [0,0,0], & v_2 &= [3,0,0],\\\\\n",
    "q_3 &= [5,6,7], & k_3 &= [2,2,0], & v_3 &= [1,2,2].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We want the self-attention output $z_1$ for token $w_1$ (*bagel*).\n",
    "\n",
    "---\n",
    "\n",
    "### (a) Unnormalized attention scores for $w_1$\n",
    "\n",
    "First compute dot products $q_1 \\cdot k_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_{11} &= q_1 \\cdot k_1 = 1\\cdot1 + 2\\cdot1 + 3\\cdot1 = 6,\\\\[2pt]\n",
    "a_{12} &= q_1 \\cdot k_2 = 1\\cdot0 + 2\\cdot0 + 3\\cdot0 = 0,\\\\[2pt]\n",
    "a_{13} &= q_1 \\cdot k_3 = 1\\cdot2 + 2\\cdot2 + 3\\cdot0 = 2 + 4 + 0 = 6.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Scale by $\\sqrt{\\lvert k_1 \\rvert}$.  \n",
    "The key vectors have dimension $3$, so $\\lvert k_1 \\rvert = 3$, hence\n",
    "\n",
    "$$\n",
    "\\sqrt{\\lvert k_1 \\rvert} = \\sqrt{3} \\approx 2 \\quad (\\text{given to round to } 2).\n",
    "$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_{11} &= \\frac{6}{2} = 3,\\\\\n",
    "a_{12} &= \\frac{0}{2} = 0,\\\\\n",
    "a_{13} &= \\frac{6}{2} = 3.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Normalize attention weights for row $i=1$\n",
    "\n",
    "Compute the row sum\n",
    "\n",
    "$$\n",
    "\\sum_{k} a_{1k} = 3 + 0 + 3 = 6.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha_{11} &= \\frac{a_{11}}{\\sum_k a_{1k}} = \\frac{3}{6} = 0.5,\\\\[2pt]\n",
    "\\alpha_{12} &= \\frac{a_{12}}{\\sum_k a_{1k}} = \\frac{0}{6} = 0,\\\\[2pt]\n",
    "\\alpha_{13} &= \\frac{a_{13}}{\\sum_k a_{1k}} = \\frac{3}{6} = 0.5.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So the normalized attention weights for $w_1$ are\n",
    "\n",
    "$$\n",
    "\\alpha_{1\\cdot} = [0.5,\\; 0,\\; 0.5].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### (c) Weighted sum of value vectors\n",
    "\n",
    "Now compute\n",
    "\n",
    "$$\n",
    "z_1 = \\sum_{j=1}^3 \\alpha_{1j} v_j\n",
    "    = \\alpha_{11} v_1 + \\alpha_{12} v_2 + \\alpha_{13} v_3.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha_{11} v_1 &= 0.5 [2,0,1] = [1,0,0.5],\\\\[2pt]\n",
    "\\alpha_{12} v_2 &= 0 [3,0,0] = [0,0,0],\\\\[2pt]\n",
    "\\alpha_{13} v_3 &= 0.5 [1,2,2] = [0.5,1,1].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Add them:\n",
    "\n",
    "$$\n",
    "z_1 = [1,0,0.5] + [0,0,0] + [0.5,1,1] = [1.5,\\; 1,\\; 1.5].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Final answer**\n",
    "\n",
    "$$\n",
    "\\boxed{z_1 = [1.5,\\; 1,\\; 1.5]}\n",
    "$$\n",
    "\n",
    "for the token *bagel*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
